package checker

import (
	"context"
	"fmt"
	"net/url"
	"strings"
	"time"

	"github.com/chromedp/chromedp"
)

// JSCrawlOptions extends CrawlOptions with JavaScript-specific settings.
type JSCrawlOptions struct {
	CrawlOptions
	EnableJavaScript bool
	WaitTime         time.Duration // Time to wait for JavaScript to render
}

// DiscoverInScopeLinksJS crawls JavaScript-rendered pages using a headless browser.
// It discovers links that are dynamically generated by JavaScript frameworks (React, Vue, Angular, etc.).
func DiscoverInScopeLinksJS(ctx context.Context, startURL string, opts JSCrawlOptions) ([]string, error) {
	if !opts.EnableJavaScript {
		// Fall back to static crawler if JS is not enabled
		return DiscoverInScopeLinks(ctx, startURL, opts.CrawlOptions)
	}

	if opts.MaxDepth <= 0 || opts.MaxPages <= 0 {
		return nil, nil
	}

	if opts.WaitTime <= 0 {
		opts.WaitTime = 2 * time.Second
	}

	target := ParseTarget(startURL)
	if target == nil || target.FullURL == "" {
		return nil, fmt.Errorf("invalid start url %q", startURL)
	}

	root, err := url.Parse(target.FullURL)
	if err != nil {
		return nil, err
	}

	// Create chromedp context
	allocCtx, allocCancel := chromedp.NewExecAllocator(ctx,
		append(chromedp.DefaultExecAllocatorOptions[:],
			chromedp.Flag("headless", true),
			chromedp.Flag("disable-gpu", true),
			chromedp.Flag("no-sandbox", true),
			chromedp.Flag("disable-dev-shm-usage", true),
		)...,
	)
	defer allocCancel()

	browserCtx, browserCancel := chromedp.NewContext(allocCtx)
	defer browserCancel()

	type queueItem struct {
		url   *url.URL
		depth int
	}

	queue := []queueItem{{url: root, depth: 0}}
	seen := map[string]struct{}{canonicalURL(root): {}}
	discovered := make([]string, 0, opts.MaxPages)

	for len(queue) > 0 && len(discovered) < opts.MaxPages {
		if err := ctx.Err(); err != nil {
			return discovered, err
		}

		item := queue[0]
		queue = queue[1:]

		if item.depth >= opts.MaxDepth {
			continue
		}

		// Fetch page and extract links using headless browser
		links, err := fetchPageWithJS(browserCtx, item.url.String(), opts.WaitTime)
		if err != nil {
			continue
		}

		for _, raw := range links {
			u, err := url.Parse(raw)
			if err != nil {
				continue
			}
			if opts.SameHostOnly && !hostsMatch(root, u) {
				continue
			}
			if looksLikeAsset(u.Path) {
				continue
			}
			key := canonicalURL(u)
			if key == "" {
				continue
			}
			if _, ok := seen[key]; ok {
				continue
			}
			seen[key] = struct{}{}
			discovered = append(discovered, key)
			if len(discovered) >= opts.MaxPages {
				break
			}
			if item.depth+1 < opts.MaxDepth {
				queue = append(queue, queueItem{url: u, depth: item.depth + 1})
			}
		}
	}

	return discovered, nil
}

// fetchPageWithJS fetches a page using headless Chrome and extracts all links after JavaScript execution.
func fetchPageWithJS(ctx context.Context, targetURL string, waitTime time.Duration) ([]string, error) {
	var htmlContent string
	var links []string

	// Navigate to page and wait for JavaScript to render
	err := chromedp.Run(ctx,
		chromedp.Navigate(targetURL),
		chromedp.Sleep(waitTime),
		chromedp.Evaluate(`Array.from(document.querySelectorAll('a[href]')).map(a => a.href)`, &links),
		chromedp.InnerHTML("body", &htmlContent, chromedp.ByQuery),
	)
	if err != nil {
		return nil, err
	}

	// Also extract links from React Router and other SPA frameworks
	var reactLinks []string
	_ = chromedp.Run(ctx,
		chromedp.Evaluate(`
			(function() {
				var links = [];
				// Extract from data attributes commonly used in SPAs
				document.querySelectorAll('[data-href], [data-url], [data-route]').forEach(el => {
					var href = el.getAttribute('data-href') || el.getAttribute('data-url') || el.getAttribute('data-route');
					if (href) links.push(href);
				});
				// Extract from onclick handlers that might contain routes
				document.querySelectorAll('[onclick]').forEach(el => {
					var onclick = el.getAttribute('onclick');
					var match = onclick.match(/['"]([\/][^'"]*)['"]/);
					if (match) links.push(match[1]);
				});
				return links;
			})()
		`, &reactLinks),
	)

	// Merge all discovered links
	allLinks := append(links, reactLinks...)

	// Resolve relative URLs
	base, err := url.Parse(targetURL)
	if err != nil {
		return allLinks, nil
	}

	resolvedLinks := make([]string, 0, len(allLinks))
	for _, link := range allLinks {
		if resolved := resolveLink(base, link); resolved != nil {
			resolvedLinks = append(resolvedLinks, resolved.String())
		}
	}

	return resolvedLinks, nil
}

// DiscoverInScopeLinksAuto automatically detects if a page requires JavaScript and uses the appropriate crawler.
func DiscoverInScopeLinksAuto(ctx context.Context, startURL string, opts JSCrawlOptions) ([]string, error) {
	// First, try static crawl
	staticLinks, err := DiscoverInScopeLinks(ctx, startURL, opts.CrawlOptions)
	if err == nil && len(staticLinks) > 0 {
		// Static crawl was successful, return results
		return staticLinks, nil
	}

	// If static crawl found nothing, check if page needs JavaScript
	needsJS, err := pageRequiresJavaScript(ctx, startURL)
	if err != nil || !needsJS {
		// Return static results even if empty
		return staticLinks, err
	}

	// Page requires JavaScript, use JS crawler
	opts.EnableJavaScript = true
	return DiscoverInScopeLinksJS(ctx, startURL, opts)
}

// pageRequiresJavaScript checks if a page requires JavaScript by looking for common SPA indicators.
func pageRequiresJavaScript(ctx context.Context, targetURL string) (bool, error) {
	target := ParseTarget(targetURL)
	if target == nil || target.FullURL == "" {
		return false, fmt.Errorf("invalid url %q", targetURL)
	}

	parsed, err := url.Parse(target.FullURL)
	if err != nil {
		return false, err
	}

	body, contentType, err := fetchPage(ctx, nil, parsed.String())
	if err != nil || !isHTML(contentType) {
		return false, err
	}

	htmlStr := strings.ToLower(string(body))

	// Check for common SPA framework indicators
	spaIndicators := []string{
		"<div id=\"root\"",
		"<div id=\"app\"",
		"<div id=\"__next",
		"you need to enable javascript",
		"<noscript",
		"react",
		"vue",
		"angular",
		"webpack",
		"vue-router",
		"react-router",
		"ng-app",
	}

	spaCount := 0
	for _, indicator := range spaIndicators {
		if strings.Contains(htmlStr, indicator) {
			spaCount++
		}
	}

	// Count actual navigation links (not just any href)
	linkMatches := hrefPattern.FindAll(body, -1)
	navigationLinks := 0
	for _, match := range linkMatches {
		href := string(match)
		// Skip asset links
		if !strings.Contains(href, ".css") && !strings.Contains(href, ".js") &&
		   !strings.Contains(href, ".png") && !strings.Contains(href, ".jpg") {
			navigationLinks++
		}
	}

	// Strong indicator: Very small HTML (< 2KB) with script tags but almost no content
	isMinimalShell := len(body) < 2048 && strings.Contains(htmlStr, "<script") && strings.Contains(htmlStr, "src=")

	// If we find SPA indicators and very few navigation links, or it's a minimal shell, it's likely a SPA
	return (spaCount >= 2 && navigationLinks < 5) || isMinimalShell, nil
}
